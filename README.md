# ğŸ® AI-Powered Tic Tac Toe Game

A sophisticated Tic Tac Toe game featuring a reinforcement learning AI opponent and a modern web interface. This project combines machine learning, game theory, and web development to create an engaging gaming experience.

![Tic Tac Toe Game](https://img.shields.io/badge/Game-Tic%20Tac%20Toe-blue) ![AI](https://img.shields.io/badge/AI-Reinforcement%20Learning-green) ![Python](https://img.shields.io/badge/Python-3.8+-blue) ![Flask](https://img.shields.io/badge/Flask-Web%20API-red)

## ğŸŒŸ Features

### ğŸ¤– **AI Components**
- **Deep Q-Network (DQN)** trained using Stable Baselines3
- **Multiple AI Strategies**: Random, Smart, Minimax, and Adaptive opponents
- **Advanced Game Logic**: Fork detection, strategic positioning, and tactical play
- **Reinforcement Learning Environment** compatible with OpenAI Gymnasium

### ğŸ¯ **Game Modes**
- **1 vs 1**: Two players with custom names
- **1 vs Bot**: Play against the trained AI opponent
- **Real-time Turn Display**: Shows whose turn it is with player names
- **Personalized Results**: Winner announcements by player name

### ğŸ¨ **Modern Web Interface**
- **Intuitive Design**: Clean, modern UI with gradient backgrounds
- **Responsive Layout**: Works on desktop and mobile devices
- **Interactive Board**: Click-to-play with visual feedback
- **Mode Selection**: Easy switching between game modes
- **Custom Player Names**: Personalize your gaming experience


## ğŸš€ Quick Start

### Prerequisites
- Python 3.8 or higher
- pip package manager
- Web browser for the frontend

### Installation & Running

1. **Clone the repository**
   ```bash
   git clone https://github.com/uselessbruh/tic-tac-toe.git
   cd ai-tictactoe
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Train the AI (Optional - pre-trained model included)**
   ```bash
   python train_agent.py
   ```

4. **Start the backend (Flask API server)**
   ```bash
   python app.py
   # By default, runs on http://localhost:5000
   ```

5. **Start the frontend (HTML server)**
   - You must serve `index.html` using a local web server (not by double-clicking the file), otherwise browser security will block API requests.
   - You can use Python's built-in HTTP server:
     ```bash
     # In the project directory (where index.html is located):
     python -m http.server 8000
     # Now open http://localhost:8000 in your browser
     ```
   - The frontend is configured to call the backend API at `http://localhost:5000/move`.
   - You can run the frontend server on any port (e.g., 8000, 3000, etc.), but the backend must be running on port 5000 for the API calls to work (or update the JS code if you change the backend port).

**Note:**
- If you open `index.html` directly as a file (file://...), the browser will block API requests to the backend. Always use a local server for the frontend.
- CORS is enabled in the backend to allow cross-origin requests from your frontend server.

## ğŸ® How to Play

1. **Choose Game Mode**
   - Select "1 vs 1" for two-player mode
   - Select "1 vs Bot" to play against AI

2. **Enter Player Names**
   - For 1v1: Enter both player names
   - For vs Bot: Enter your name

3. **Start Playing**
   - Click "Start Game" to begin
   - Click on board squares to make moves
   - Follow turn indicators

4. **Game Results**
   - Winner displayed by name
   - "Restart" to play again with different settings

## ğŸ—ï¸ Project Structure

```
ai-tictactoe/
â”œâ”€â”€ app.py                 # Flask web server and API
â”œâ”€â”€ index.html            # Modern web interface
â”œâ”€â”€ tictactoe_env.py      # RL environment with multiple AI strategies
â”œâ”€â”€ train_agent.py        # AI training script
â”œâ”€â”€ test.py              # Model testing script
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ tictactoe_agent.zip   # Pre-trained AI model
â””â”€â”€ README.md            # This file
```

## ğŸ§  AI Architecture

### Reinforcement Learning Environment
- **State Space**: 18-dimensional (9 board positions + 9 valid move mask)
- **Action Space**: 9 discrete actions (board positions 0-8)
- **Reward System**: 
  - +10 for winning
  - -5 for losing
  - +1 for draws
  - -10 for invalid moves
  - Small positional rewards for strategic play

### AI Opponent Strategies

#### ğŸ¯ **Smart Strategy**
1. **Win immediately** if possible
2. **Block opponent** from winning
3. **Create forks** (multiple winning paths)
4. **Block opponent forks**
5. **Take center** for strategic advantage
6. **Take opposite corners**
7. **Prefer corners** over edges
8. **Strategic positioning** based on evaluation

#### ğŸ§® **Minimax Strategy**
- Perfect play using minimax algorithm with alpha-beta pruning
- Guaranteed optimal moves (never loses when playing optimally)

#### ğŸ”„ **Adaptive Strategy**
- Learns from player patterns
- Mixes strategies based on game history
- Increases difficulty over time

## ğŸ”§ API Reference

### Endpoints

#### `POST /move`
Get AI move for current board state.

**Request Body:**
```json
{
  "board": [0, 1, 0, 2, 1, 0, 0, 0, 2]
}
```

**Response:**
```json
{
  "move": 6
}
```

**Board Encoding:**
- `0`: Empty cell
- `1`: Player X
- `2`: Player O (Bot)

## ğŸ› ï¸ Development

### Training Your Own Model

```python
from stable_baselines3 import DQN
from tictactoe_env import SmartTicTacToeEnv

# Create environment with desired opponent strategy
env = SmartTicTacToeEnv(opponent_strategy='smart')

# Train model
model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100_000)

# Save model
model.save("your_model_name")
```

### Testing the Model

```python
from stable_baselines3 import DQN
from tictactoe_env import SmartTicTacToeEnv

# Load model
model = DQN.load("tictactoe_agent")
env = SmartTicTacToeEnv()

# Test gameplay
obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, _, _ = env.step(action)
    env.render()
```

### Customizing AI Difficulty

Modify the opponent strategy in `tictactoe_env.py`:
- `'random'`: Easy (random moves)
- `'smart'`: Medium (strategic but not perfect)
- `'minimax'`: Hard (optimal play)
- `'adaptive'`: Dynamic (learns and adapts)

## ğŸ“Š Performance Metrics

The trained AI achieves:
- **95%+ win rate** against random opponents
- **60-70% win/draw rate** against smart opponents
- **50% draw rate** against minimax (optimal play)
- **Sub-second response time** for move calculations

## ğŸš€ Deployment

### Local Development
```bash
python app.py
# Access at http://localhost:5000
```

### Production Deployment
For production deployment, consider:
- Using Gunicorn or uWSGI for serving Flask
- Setting up reverse proxy with Nginx
- Using environment variables for configuration
- Implementing proper logging and monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guidelines
- Add tests for new features
- Update documentation as needed
- Ensure AI training convergence before committing models

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Stable Baselines3** for the DQN implementation
- **OpenAI Gymnasium** for the RL environment framework
- **Flask** for the web API framework
- **NumPy** for numerical computations

## ğŸ› Known Issues

- AI model requires retraining if environment parameters change significantly
- Web interface requires manual refresh after network errors
- Training time scales with complexity of opponent strategy

## ğŸ”® Future Enhancements

- [ ] Neural network visualization for AI decision making
- [ ] Online multiplayer with WebSocket support
- [ ] Tournament mode with multiple AI opponents
- [ ] Mobile app version
- [ ] AI vs AI battle mode
- [ ] Advanced statistics and analytics
- [ ] Custom board sizes (4x4, 5x5)
- [ ] AI difficulty slider with real-time adjustment

## ğŸ“§ Contact

For questions, suggestions, or collaboration opportunities, please open an issue or reach out via:
- GitHub Issues: [Create an issue](https://github.com/uselessbruh/tic-tac-toee/issues)

---

**Enjoy playing against our AI! Can you beat the machine? ğŸ¤–ğŸ¯**
